# -*- coding: utf-8 -*-
"""Fusion AI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y5V7uNEVMJMCilV7glpD2dBsHD8B42bK

# **Stage 1**

## **Importing Dependencies**
"""

!pip install -q transformers

"""## **Load DialoGPT and Set Fusion AI Personality**"""

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Fusion AI identity
BOT_NAME = "Fusion AI"
BOT_PERSONALITY = "a friendly, emotionally intelligent AI who chats like a caring best friend."

# Load DialoGPT-medium
model_name = "microsoft/DialoGPT-medium"
tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Setup chat history
chat_history_ids = None
step = 0

print(f"‚úÖ {BOT_NAME} is ready to talk!")
print(f"üí° Personality: {BOT_PERSONALITY}")

"""## **Define Chat Function**"""

# Create a message queue and control cell
def fusion_chat_manual(user_input):
    global chat_history_ids, step

    if user_input.lower() in ["exit", "quit", "bye"]:
        return f"üëã {BOT_NAME}: Take care! I'm always here if you need me."

    # Encode the new user input
    new_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt')

    # Append user input to chat history
    bot_input_ids = (
        torch.cat([chat_history_ids, new_input_ids], dim=-1)
        if step > 0 else new_input_ids
    )

    # Generate response
    chat_history_ids = model.generate(
        bot_input_ids,
        max_length=1000,
        pad_token_id=tokenizer.eos_token_id,
        do_sample=True,
        top_k=50,
        top_p=0.95,
        temperature=0.75,
    )

    # Decode bot output
    bot_output = tokenizer.decode(
        chat_history_ids[:, bot_input_ids.shape[-1]:][0],
        skip_special_tokens=True
    )

    step += 1
    return f"ü§ñ {BOT_NAME}: {bot_output}"

# Replace this string with your message
fusion_chat_manual("I‚Äôm feeling really anxious lately.")

"""# **Stage 2**

## **Importing Dependencies**
"""

!pip install transformers

"""## **Load Emotion Detection Model**"""

from transformers import pipeline

# Load the emotion detection pipeline (Hugging Face model)
emotion_classifier = pipeline("text-classification", model="j-hartmann/emotion-english-distilroberta-base")

# This will analyze emotions like joy, sadness, anger, etc.

"""## **Define Emotion-Aware Chat Function**"""

def detect_emotion(user_input):
    # Classify the emotion of the user's message
    emotion = emotion_classifier(user_input)[0]['label']
    return emotion

def fusion_chat_with_emotion(user_input):
    global chat_history_ids, step

    # Detect emotion from the user's input
    emotion = detect_emotion(user_input)
    print(f"üí° Detected Emotion: {emotion}")

    # Encode user input
    new_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt')

    # Append user input to chat history
    bot_input_ids = (
        torch.cat([chat_history_ids, new_input_ids], dim=-1)
        if step > 0 else new_input_ids
    )

    # Generate response
    chat_history_ids = model.generate(
        bot_input_ids,
        max_length=1000,
        pad_token_id=tokenizer.eos_token_id,
        do_sample=True,
        top_k=50,
        top_p=0.95,
        temperature=0.75,
    )

    # Decode bot output
    bot_output = tokenizer.decode(
        chat_history_ids[:, bot_input_ids.shape[-1]:][0],
        skip_special_tokens=True
    )

    # Modify Fusion AI's tone based on emotion
    if emotion == "joy":
        bot_output = f"ü§ñ {BOT_NAME}: Yay! I'm so glad you're feeling joyful! üòä {bot_output}"
    elif emotion == "sadness":
        bot_output = f"ü§ñ {BOT_NAME}: Oh no, I‚Äôm here for you. It‚Äôs okay to feel down sometimes. üíô {bot_output}"
    elif emotion == "anger":
        bot_output = f"ü§ñ {BOT_NAME}: I understand you're feeling frustrated. Let's take a deep breath together. üßò‚Äç‚ôÄÔ∏è {bot_output}"
    elif emotion == "fear":
        bot_output = f"ü§ñ {BOT_NAME}: Hey, it‚Äôs okay to feel scared. You‚Äôre not alone, I‚Äôve got you! ü§ó {bot_output}"
    else:
        bot_output = f"ü§ñ {BOT_NAME}: I hear you. Let‚Äôs chat and figure this out together! {bot_output}"

    # Increase step count for chat history
    step += 1

    return bot_output

"""## **Test the Emotion-Aware Chatbot**"""

# Test emotion-aware chatbot
user_input = input("üßë You: ")
response = fusion_chat_with_emotion(user_input)
print(response)

"""# **Stage 3**

## **Implementing Advanced Personalization & Knowledge Storage**
"""

import json

# Simple database to store user information and preferences
def save_user_data(data, filename="user_data.json"):
    """Save user data to a JSON file"""
    with open(filename, 'w') as f:
        json.dump(data, f)

def load_user_data(filename="user_data.json"):
    """Load user data from a JSON file"""
    with open(filename, 'r') as f:
        return json.load(f)

# Example structure for user data
user_data = {
    "name": "User",
    "preferences": {
        "favorite_color": "blue",
        "hobbies": ["reading", "coding", "music"],
        "favorite_food": "pizza",
    }
}

# Save example data to JSON file
save_user_data(user_data)

"""## **Emotion Detection & Contextual Response Generation**"""

from textblob import TextBlob

# Function to detect emotion (basic sentiment analysis)
def detect_emotion(text):
    analysis = TextBlob(text)
    sentiment = analysis.sentiment.polarity  # -1 to 1 scale
    if sentiment > 0.2:
        return "joy"
    elif sentiment < -0.2:
        return "sadness"
    else:
        return "neutral"

# Enhanced function to generate personalized and emotionally aware response
def fusion_chat_with_personalization(user_input):
    data = load_user_data()
    name = data["name"]
    preferences = data["preferences"]

    # Detect user emotion
    emotion = detect_emotion(user_input)

    # Generate personalized response based on emotion
    if emotion == "joy":
        response = f"ü§ñ Fusion AI: Hey {name}, I see you're feeling happy! How about we talk about your favorite hobby, {preferences['hobbies'][0]}?"
    elif emotion == "sadness":
        response = f"ü§ñ Fusion AI: I'm here for you, {name}. How about we talk about something comforting, like {preferences['favorite_food']}?"
    else:
        response = f"ü§ñ Fusion AI: Got it, {name}. Let's dive into something exciting, like {preferences['hobbies'][1]}!"

    return response

"""## **Continuous Learning (Adding New Information)**"""

# Function to teach Fusion AI new information
def teach_fusion_ai(new_knowledge):
    # Load current user data
    data = load_user_data()

    # Add new knowledge (example: a new hobby or favorite color)
    if "new_hobby" in new_knowledge:
        data["preferences"]["hobbies"].append(new_knowledge["new_hobby"])

    # Save updated data back to JSON
    save_user_data(data)
    return f"ü§ñ Fusion AI: Got it, {data['name']}! I've added your new hobby: {new_knowledge['new_hobby']}!"

# Example of adding new hobby
new_knowledge = {"new_hobby": "coding"}
teach_fusion_ai(new_knowledge)

"""## **Combining Emotional Awareness with DialoGPT**"""

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# Load pre-trained DialoGPT model
model_name = "microsoft/DialoGPT-medium"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Function to integrate DialoGPT with personalized responses
def fusion_chat_with_dialo(user_input):
    data = load_user_data()
    name = data["name"]
    preferences = data["preferences"]

    # Detect user emotion from input
    emotion = detect_emotion(user_input)
    personalized_response = fusion_chat_with_personalization(user_input)

    # Generate conversation response using DialoGPT
    new_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt')
    bot_input_ids = new_input_ids

    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id, do_sample=True, top_k=50, top_p=0.95, temperature=0.75)
    bot_output = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)

    # Combine personalized response with generated response
    full_response = f"{personalized_response}\n{bot_output}"
    return full_response

"""## **Custom Knowledge Integration (Add Facts & Details)**"""

# Function to add a custom fact
def add_custom_fact(fact):
    data = load_user_data()

    # Store new facts
    if "custom_facts" not in data:
        data["custom_facts"] = []

    data["custom_facts"].append(fact)
    save_user_data(data)

    return f"ü§ñ Fusion AI: I've saved this fact: {fact}"

# Example of adding a custom fact
fact = "Fusion AI loves helping with coding!"
add_custom_fact(fact)

"""# **Stage 4**

## **Importing Dependencies**
"""

!pip install requests

"""## **Code for fetching the latest news**"""

import requests

# Function to fetch current affairs from NewsAPI
def get_current_affairs(category="general"):
    api_key = 'your_newsapi_key'  # Replace with your actual API key
    url = f'https://newsapi.org/v2/top-headlines?country=us&category={category}&apiKey={api_key}'
    response = requests.get(url)

    # Print the full response for debugging
    print(response.json())  # This will show the response structure

    news = response.json()
    if news.get("status") == "ok":
        articles = news["articles"]
        headlines = [article["title"] for article in articles]
        return "\n".join(headlines[:5])  # Get top 5 headlines
    else:
        return "Sorry, I couldn't fetch the latest news right now."

"""## **Install & Load Tools**"""

!pip install -q transformers huggingface_hub

"""## **Use a Free Coding Model from Hugging Face**"""

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# FREE and public coding model
model_name = "Salesforce/codegen-350M-mono"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map="auto")

"""## **Coding Assistant Function**"""

def fusion_code_assistant(prompt):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=256, do_sample=True, top_p=0.9, temperature=0.7)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response[len(prompt):].strip()

"""## **Free News via Wikipedia**"""

!pip install wikipedia

"""## **Add Wikipedia Knowledge Retrieval**"""

import wikipedia

# Set the language (optional, default is "en")
wikipedia.set_lang("en")

def get_wiki_info(query):
    try:
        summary = wikipedia.summary(query, sentences=3)
        return f"üåê According to Wikipedia:\n{summary}"
    except wikipedia.DisambiguationError as e:
        return f"‚ö†Ô∏è Too many results! Be more specific. Options include:\n{e.options[:5]}"
    except wikipedia.PageError:
        return "‚ùå Sorry, I couldn't find anything on that topic."
    except Exception as e:
        return f"üö´ An error occurred: {str(e)}"

"""## **Integrate Wikipedia into Fusion AI‚Äôs Brain**"""

def fusion_ai(prompt):
    prompt_lower = prompt.lower()

    if "news" in prompt_lower:
        return f"üì∞ Latest Headlines:\n{get_latest_headlines()}"
    elif "code" in prompt_lower or "python" in prompt_lower:
        return f"üíª Here's some help with your coding:\n{fusion_code_assistant(prompt)}"
    elif "who is" in prompt_lower or "what is" in prompt_lower or "tell me about" in prompt_lower:
        return get_wiki_info(prompt)
    else:
        return f"üí¨ Fusion AI: I‚Äôm here, always listening. You said: {prompt}"

"""## **Chat Loop**"""

import wikipedia
import random

# Set Wikipedia language to English
wikipedia.set_lang("en")

# Emotion engine
def detect_emotion(text):
    text = text.lower()
    if any(word in text for word in ["sad", "upset", "tired", "depressed"]):
        return random.choice([
            "I'm here for you. You're never alone ü§ó",
            "It's okay to feel this way. I'm listening ü´Ç",
            "You've got this. I'm cheering for you üí™"
        ])
    elif any(word in text for word in ["happy", "excited", "joy", "love"]):
        return random.choice([
            "Yay! That makes me so happy too! üòÑ",
            "Let‚Äôs celebrate that amazing feeling! üéâ",
            "Your joy lights me up! üíñ"
        ])
    return None

# Wikipedia fetcher
def get_wiki_info(query):
    try:
        summary = wikipedia.summary(query, sentences=3)
        return f"üåê According to Wikipedia:\n{summary}"
    except wikipedia.DisambiguationError as e:
        return f"‚ö†Ô∏è Be more specific. Did you mean: {', '.join(e.options[:3])}?"
    except wikipedia.PageError:
        return "‚ùå Sorry, I couldn‚Äôt find anything on that topic."
    except Exception as e:
        return f"üö´ An error occurred: {str(e)}"

# Dummy coding assistant
def fusion_code_assistant(prompt):
    if "for loop" in prompt.lower():
        return "Here's a Python for loop:\n```python\nfor i in range(5):\n    print(i)\n```"
    elif "if statement" in prompt.lower():
        return "Here's a Python if-else:\n```python\nif x > 0:\n    print('Positive')\nelse:\n    print('Not positive')\n```"
    else:
        return "I'm still learning to code like ChatGPT! Ask me basic Python questions üòÑ"

# Fusion AI Core
def fusion_ai(prompt):
    emotion = detect_emotion(prompt)
    if emotion:
        return emotion
    elif any(kw in prompt.lower() for kw in ["who is", "what is", "tell me about"]):
        return get_wiki_info(prompt)
    elif any(kw in prompt.lower() for kw in ["python", "code", "loop", "function", "class"]):
        return fusion_code_assistant(prompt)
    else:
        return f"ü§ñ Fusion AI: I hear you. You said: {prompt}"

# Start chat loop
print("üëã Welcome to Fusion AI! Type 'exit' to quit.\n")

while True:
    user_input = input("You: ")
    if user_input.lower() in ['exit', 'quit']:
        print("üëã Goodbye! Fusion AI will miss you!")
        break
    response = fusion_ai(user_input)
    print(response)

"""# **Stage 5**

## **Coding Assistant Setup**
"""

# üß† Coding Assistant: Replies with code snippets in different languages
def fusion_multilang_code(prompt):
    prompt = prompt.lower()

    if "python" in prompt:
        return "üß™ Python Example:\n```python\ndef greet(name):\n    print(f\"Hello, {name}!\")\n```"

    elif "c++" in prompt or "cpp" in prompt:
        return "üõ†Ô∏è C++ Example:\n```cpp\nfor(int i = 0; i < 5; i++) {\n    std::cout << i << std::endl;\n}\n```"

    elif "java" in prompt:
        return "‚òï Java Example:\n```java\npublic class Hello {\n    public static void main(String[] args) {\n        System.out.println(\"Hello, World!\");\n    }\n}\n```"

    elif "html" in prompt:
        return "üåê HTML Example:\n```html\n<!DOCTYPE html>\n<html>\n<head><title>Title</title></head>\n<body>\n    <h1>Hello!</h1>\n</body>\n</html>\n```"

    elif "javascript" in prompt or "js" in prompt:
        return "‚öôÔ∏è JavaScript Example:\n```javascript\nfunction greet(name) {\n    console.log(\"Hello, \" + name);\n}\n```"

    elif "sql" in prompt:
        return "üíæ SQL Example:\n```sql\nSELECT * FROM users WHERE age > 18;\n```"

    else:
        return "ü§î I couldn‚Äôt detect the language. Try: _Show me a for loop in Python_ or _HTML structure example_"

"""## **Add Code Explanation & Smart Language Detection**"""

# üìö Advanced Code Assistant with Explanation
def fusion_multilang_code(prompt):
    prompt_lower = prompt.lower()

    if "python" in prompt_lower:
        return (
            "üêç Python Example:\n"
            "```python\nnumbers = [5, 2, 9, 1]\nnumbers.sort()\nprint(numbers)\n```\n"
            "üß† Explanation: This sorts a list of numbers in ascending order using the built-in `.sort()` method."
        )

    elif "c++" in prompt_lower or "cpp" in prompt_lower:
        return (
            "üõ†Ô∏è C++ Example:\n"
            "```cpp\n#include <iostream>\nusing namespace std;\n\nint main() {\n    cout << \"Hello World!\";\n    return 0;\n}\n```\n"
            "üß† Explanation: This is a basic C++ program that prints 'Hello World!' to the console."
        )

    elif "java" in prompt_lower:
        return (
            "‚òï Java Example:\n"
            "```java\npublic class Hello {\n    public static void main(String[] args) {\n        System.out.println(\"Hi there!\");\n    }\n}\n```\n"
            "üß† Explanation: This Java class prints a message to the screen."
        )

    elif "html" in prompt_lower:
        return (
            "üåê HTML Example:\n"
            "```html\n<!DOCTYPE html>\n<html>\n<head><title>Page</title></head>\n<body><h1>Hello</h1></body>\n</html>\n```\n"
            "üß† Explanation: This creates a simple HTML webpage with a heading."
        )

    elif "javascript" in prompt_lower or "js" in prompt_lower:
        return (
            "‚öôÔ∏è JavaScript Example:\n"
            "```javascript\nfunction greet(name) {\n    console.log('Hi ' + name);\n}\n```\n"
            "üß† Explanation: This defines a JS function to greet a user by name in the browser console."
        )

    elif "sql" in prompt_lower:
        return (
            "üíæ SQL Example:\n"
            "```sql\nSELECT name FROM students WHERE grade > 80;\n```\n"
            "üß† Explanation: This SQL query retrieves names of students who scored above 80."
        )

    else:
        return "ü§î I couldn't detect the coding language. Try something like: _Python loop example_ or _HTML page example_."

"""## **Execute & Explain Python Code**

### **Add Required Python Module**
"""

import contextlib
import io
import traceback

"""### **Add the Python Code Runner Function**"""

# üöÄ Execute Python Code & Explain It
def execute_python_code(user_code):
    try:
        buffer = io.StringIO()
        with contextlib.redirect_stdout(buffer):
            exec(user_code, {})
        output = buffer.getvalue()
        return f"‚úÖ Output:\n```\n{output.strip()}\n```"
    except Exception as e:
        error_msg = traceback.format_exc()
        return f"‚ùå Error:\n```\n{error_msg}\n```"

"""### **Detect and Handle Python Code in the Chat Loop**"""

def fusion_ai():
    print("ü§ñ Fusion AI: Hello, I'm Fusion AI! Your emotional, intelligent, and code-savvy buddy. Ask me anything.\n")

    while True:
        prompt = input("üß† You: ")

        # üõë Exit condition
        if prompt.lower() in ["exit", "quit", "bye"]:
            print("ü§ñ Fusion AI: Goodbye, my friend. Come back soon üíô")
            break

        # üëÅÔ∏è Detect Python code snippet
        if "run python" in prompt.lower() or prompt.strip().startswith("```python"):
            code = prompt.replace("```python", "").replace("```", "").replace("run python", "").strip()
            response = execute_python_code(code)

        else:
            # üß† Fallback to base conversational model
            inputs = tokenizer.encode(prompt + tokenizer.eos_token, return_tensors="pt")
            reply_ids = model.generate(inputs, max_length=1000, pad_token_id=tokenizer.eos_token_id)
            response = tokenizer.decode(reply_ids[:, inputs.shape[-1]:][0], skip_special_tokens=True)

        print(f"\nü§ñ Fusion AI: {response}\n")

"""## **Explain Python Code**

### **Add the Code Explanation Function**
"""

import ast

# Function to get a simple explanation of Python code
def explain_python_code(user_code):
    try:
        tree = ast.parse(user_code)
        explanation = ""
        for node in tree.body:
            if isinstance(node, ast.FunctionDef):
                explanation += f"This is a function definition: `{node.name}`\n"
            elif isinstance(node, ast.For):
                explanation += "This is a `for` loop.\n"
            elif isinstance(node, ast.If):
                explanation += "This is an `if` condition.\n"
            elif isinstance(node, ast.Assign):
                explanation += "This is an assignment statement.\n"
        return explanation or "Code is simple and clean. No complex structures found!"
    except Exception as e:
        return f"‚ùå Error while explaining code: {str(e)}"

"""### **Update the Chat Loop with Explanation**"""

def fusion_ai():
    print("ü§ñ Fusion AI: Hello, I'm Fusion AI! Your emotional, intelligent, and code-savvy buddy. Ask me anything.\n")

    while True:
        prompt = input("üß† You: ")

        # üõë Exit condition
        if prompt.lower() in ["exit", "quit", "bye"]:
            print("ü§ñ Fusion AI: Goodbye, my friend. Come back soon üíô")
            break

        # üëÅÔ∏è Detect Python code snippet
        if "run python" in prompt.lower() or prompt.strip().startswith("```python"):
            code = prompt.replace("```python", "").replace("```", "").replace("run python", "").strip()

            # Explain the code first
            explanation = explain_python_code(code)
            response = f"üîç Explanation: \n{explanation}\n\n"

            # Execute the Python code
            response += execute_python_code(code)

        else:
            # üß† Fallback to base conversational model
            inputs = tokenizer.encode(prompt + tokenizer.eos_token, return_tensors="pt")
            reply_ids = model.generate(inputs, max_length=1000, pad_token_id=tokenizer.eos_token_id)
            response = tokenizer.decode(reply_ids[:, inputs.shape[-1]:][0], skip_special_tokens=True)

        print(f"\nü§ñ Fusion AI: {response}\n")

"""# **Stage 6**

## **Setup Libraries**
"""

!pip install transformers wikipedia

"""## **Import Necessary Modules**"""

from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
import wikipedia

# Initialize sentiment analyzer
sentiment_analyzer = pipeline('sentiment-analysis')

# Simple memory class to remember conversations
class ChatbotMemory:
    def __init__(self):
        self.memory = {}

    def update_memory(self, user_input, chatbot_response):
        self.memory[user_input] = chatbot_response

    def get_memory(self, user_input):
        return self.memory.get(user_input, "I don't remember that.")

"""## **Emotion Recognition Function**"""

def detect_emotion(text):
    sentiment = sentiment_analyzer(text)
    label = sentiment[0]['label']
    return label

"""## **General Knowledge Integration (Using Wikipedia)**"""

def get_knowledge_from_wikipedia(query):
    try:
        summary = wikipedia.summary(query, sentences=2)
        return summary
    except wikipedia.exceptions.DisambiguationError as e:
        return f"Sorry, there are multiple results for '{query}'. Could you be more specific?"
    except wikipedia.exceptions.HTTPError as e:
        return "Sorry, I couldn't fetch the information right now."
    except Exception as e:
        return "Sorry, I couldn't find the information you requested."

"""## **Chatbot with Context Memory and Emotional Intelligence**"""

# Load conversational model
model_name = "microsoft/DialoGPT-medium"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

def generate_chat_response(prompt):
    # Encode the input and generate a response
    new_user_input_ids = tokenizer.encode(prompt + tokenizer.eos_token, return_tensors='pt')
    bot_output = model.generate(new_user_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)
    chat_response = tokenizer.decode(bot_output[:, new_user_input_ids.shape[-1]:][0], skip_special_tokens=True)
    return chat_response

"""## **The Chat Loop (With Emotion and Memory)**"""

# Create a memory object to remember conversation
chatbot_memory = ChatbotMemory()

def chat_with_bot():
    print("ü§ñ Fusion AI: Hello! How can I help you today?")

    while True:
        user_input = input("You: ")

        # Emotion recognition
        emotion = detect_emotion(user_input)

        # Generate response based on user input
        if "exit" in user_input.lower():
            print("ü§ñ Fusion AI: Goodbye!")
            break
        elif "tell me about" in user_input.lower():
            # If the user asks for general knowledge
            query = user_input.replace("tell me about", "").strip()
            knowledge_response = get_knowledge_from_wikipedia(query)
            print(f"ü§ñ Fusion AI: {knowledge_response}")
        else:
            # Generate a conversational response
            chatbot_response = generate_chat_response(user_input)

            # Store memory of the conversation
            chatbot_memory.update_memory(user_input, chatbot_response)

            # Output the chatbot's response
            print(f"ü§ñ Fusion AI: {chatbot_response}")

        # Provide emotional feedback
        if emotion == 'POSITIVE':
            print("ü§ñ Fusion AI: You seem happy! That's great!")
        elif emotion == 'NEGATIVE':
            print("ü§ñ Fusion AI: I'm sorry you're feeling down. I'm here for you!")

# Start the chatbot interaction
chat_with_bot()

"""# **Stage 7**

## **Install Required Libraries**
"""

!pip install pyngrok streamlit transformers wikipedia

"""## **Integrating All Stages into One Python File**"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile fusion_ai_chatbot.py
# import streamlit as st
# import wikipedia
# from transformers import AutoModelForCausalLM, AutoTokenizer
# import torch
# 
# st.set_page_config(page_title="Fusion AI", page_icon="ü§ñ")
# 
# st.title("ü§ñ Fusion AI")
# st.markdown("A personal chatbot that helps you chat, feel, and code!")
# 
# # Load model
# model_name = "microsoft/DialoGPT-medium"
# tokenizer = AutoTokenizer.from_pretrained(model_name)
# model = AutoModelForCausalLM.from_pretrained(model_name)
# 
# if "chat_history_ids" not in st.session_state:
#     st.session_state.chat_history_ids = None
# if "past_inputs" not in st.session_state:
#     st.session_state.past_inputs = []
# 
# # User input
# user_input = st.text_input("You:", key="input")
# 
# if user_input:
#     st.session_state.past_inputs.append(user_input)
# 
#     # Tokenize input
#     new_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt')
# 
#     # Append to history
#     bot_input_ids = torch.cat([st.session_state.chat_history_ids, new_input_ids], dim=-1) if st.session_state.chat_history_ids is not None else new_input_ids
# 
#     # Generate response
#     st.session_state.chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)
# 
#     response = tokenizer.decode(st.session_state.chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)
# 
#     st.markdown(f"**Fusion AI:** {response}")
#

import os
import time
from pyngrok import ngrok

# Kill any previous streamlit sessions (if any)
!pkill streamlit

# Step 1: Run your Streamlit app in the background
os.system('nohup streamlit run fusion_ai_chatbot.py --server.port 8501 &')
time.sleep(5)  # Wait for it to start

# Step 2: Use ngrok correctly with `addr`
public_url = ngrok.connect(addr=8501)
print(f"üöÄ Fusion AI is live at: {public_url}")

"""# **STAGE 8**

## **Prerequisites**
"""

!pip install transformers accelerate sentencepiece streamlit pyngrok --quiet

"""## **Code for fusion_ai_app.py**"""

# fusion_ai_app.py

import streamlit as st
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
import torch

st.set_page_config(page_title="Fusion AI", page_icon="ü§ñ", layout="centered")

st.title("ü§ñ Fusion AI - Your LLM Assistant")
st.markdown("Ask anything ‚Äî from coding help to current affairs, general queries, or emotional support.")

@st.cache_resource
def load_model():
    model_name = "EleutherAI/gpt-neo-2.7B"  # ‚úÖ Open-access model without the need for login
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )
    return pipeline("text-generation", model=model, tokenizer=tokenizer, max_new_tokens=512)

llm = load_model()

# Chat interface
user_prompt = st.text_area("üí¨ You:", height=100, placeholder="Ask Fusion AI anything...")

if st.button("Generate Response"):
    if user_prompt.strip() != "":
        with st.spinner("Fusion AI is thinking..."):
            response = llm(user_prompt)[0]["generated_text"]
            # Remove prompt from start of generated text if it repeats
            if response.startswith(user_prompt):
                response = response[len(user_prompt):]
            st.markdown(f"**üß† Fusion AI:** {response.strip()}")
    else:
        st.warning("Please enter a question.")

st.markdown("""
    <style>
    .css-1v3fvcr {
        background-color: #f7f7f7;
        border-radius: 8px;
        box-shadow: 0 2px 5px rgba(0,0,0,0.1);
    }
    .css-1v3fvcr input {
        font-size: 1.2em;
    }
    .css-1v3fvcr button {
        background-color: #4CAF50;
        color: white;
        font-weight: bold;
        padding: 10px;
    }
    </style>
""", unsafe_allow_html=True)

category = st.radio("Choose a topic:", ["General Knowledge", "Coding Help", "Emotional Support"])

user_name = st.text_input("Hrishi:")
user_prompt = st.text_area(f"Hello {user_name}, ask me anything...", height=100)

import streamlit as st
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
import torch

# Set the page title and icon
st.set_page_config(page_title="Fusion AI", page_icon="ü§ñ", layout="centered")

# Title and description
st.title("ü§ñ Fusion AI - Your LLM Assistant")
st.markdown("Ask anything ‚Äî from coding help to current affairs, general queries, or emotional support.")

# Custom CSS for Dark/Light mode
dark_mode_css = """
    <style>
    body {
        background-color: #1e1e1e;
        color: white;
    }
    .css-1v3fvcr {
        background-color: #333;
        border-radius: 8px;
        box-shadow: 0 2px 5px rgba(0,0,0,0.1);
    }
    .css-1v3fvcr button {
        background-color: #4CAF50;
        color: white;
    }
    .css-1v3fvcr input {
        font-size: 1.2em;
        background-color: #333;
        color: white;
    }
    .stButton>button {
        background-color: #4CAF50;
        color: white;
        font-weight: bold;
        padding: 10px;
        border-radius: 4px;
    }
    </style>
"""
light_mode_css = """
    <style>
    body {
        background-color: #f7f7f7;
        color: black;
    }
    .css-1v3fvcr {
        background-color: #ffffff;
        border-radius: 8px;
        box-shadow: 0 2px 5px rgba(0,0,0,0.1);
    }
    .css-1v3fvcr button {
        background-color: #4CAF50;
        color: white;
    }
    .css-1v3fvcr input {
        font-size: 1.2em;
        background-color: #ffffff;
        color: black;
    }
    .stButton>button {
        background-color: #4CAF50;
        color: white;
        font-weight: bold;
        padding: 10px;
        border-radius: 4px;
    }
    </style>
"""

# Toggle between dark and light mode
mode = st.radio("Select Theme", ("Light Mode", "Dark Mode"))

if mode == "Dark Mode":
    st.markdown(dark_mode_css, unsafe_allow_html=True)
else:
    st.markdown(light_mode_css, unsafe_allow_html=True)

# Model loading function
@st.cache_resource
def load_model():
    model_name = "EleutherAI/gpt-neo-2.7B"  # ‚úÖ Open-access model without the need for login
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )
    return pipeline("text-generation", model=model, tokenizer=tokenizer, max_new_tokens=512)

llm = load_model()

# Chat interface
user_prompt = st.text_area("üí¨ You:", height=100, placeholder="Ask Fusion AI anything...")

if st.button("Generate Response"):
    if user_prompt.strip() != "":
        with st.spinner("Fusion AI is thinking..."):
            response = llm(user_prompt)[0]["generated_text"]
            # Remove prompt from start of generated text if it repeats
            if response.startswith(user_prompt):
                response = response[len(user_prompt):]
            st.markdown(f"**üß† Fusion AI:** {response.strip()}")
    else:
        st.warning("Please enter a question.")

